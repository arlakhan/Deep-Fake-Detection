{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVGXJIxT8MwSo9eeMQOSRS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arlakhan/Deep-Fake-Detection/blob/main/Blockchain%2BAI%20Watermarking%20Validation%20for%20Video%20Deep%20Fake\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdds_9lGm-qX",
        "outputId": "9e5e07bc-1b41-4ce8-c289-589a4303dd66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting web3\n",
            "  Downloading web3-7.10.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting eth-abi>=5.0.1 (from web3)\n",
            "  Downloading eth_abi-5.2.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting eth-account>=0.13.1 (from web3)\n",
            "  Downloading eth_account-0.13.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting eth-hash>=0.5.1 (from eth-hash[pycryptodome]>=0.5.1->web3)\n",
            "  Downloading eth_hash-0.7.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting eth-typing>=5.0.0 (from web3)\n",
            "  Downloading eth_typing-5.2.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting eth-utils>=5.0.0 (from web3)\n",
            "  Downloading eth_utils-5.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting hexbytes>=1.2.0 (from web3)\n",
            "  Downloading hexbytes-1.3.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: aiohttp>=3.7.4.post0 in /usr/local/lib/python3.11/dist-packages (from web3) (3.11.15)\n",
            "Requirement already satisfied: pydantic>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from web3) (2.11.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from web3) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from web3) (4.13.0)\n",
            "Collecting types-requests>=2.0.0 (from web3)\n",
            "  Downloading types_requests-2.32.0.20250328-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websockets<16.0.0,>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from web3) (15.0.1)\n",
            "Collecting pyunormalize>=15.0.0 (from web3)\n",
            "  Downloading pyunormalize-16.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3) (1.18.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Collecting parsimonious<0.11.0,>=0.10.0 (from eth-abi>=5.0.1->web3)\n",
            "  Downloading parsimonious-0.10.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting bitarray>=2.4.0 (from eth-account>=0.13.1->web3)\n",
            "  Downloading bitarray-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting eth-keyfile<0.9.0,>=0.7.0 (from eth-account>=0.13.1->web3)\n",
            "  Downloading eth_keyfile-0.8.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting eth-keys>=0.4.0 (from eth-account>=0.13.1->web3)\n",
            "  Downloading eth_keys-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting eth-rlp>=2.1.0 (from eth-account>=0.13.1->web3)\n",
            "  Downloading eth_rlp-2.2.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting rlp>=1.0.0 (from eth-account>=0.13.1->web3)\n",
            "  Downloading rlp-4.1.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting ckzg>=2.0.0 (from eth-account>=0.13.1->web3)\n",
            "  Downloading ckzg-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (887 bytes)\n",
            "Collecting pycryptodome<4,>=3.6.6 (from eth-hash[pycryptodome]>=0.5.1->web3)\n",
            "  Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting cytoolz>=0.10.1 (from eth-utils>=5.0.0->web3)\n",
            "  Downloading cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.0->web3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.0->web3) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.0->web3) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->web3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->web3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->web3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->web3) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from cytoolz>=0.10.1->eth-utils>=5.0.0->web3) (0.12.1)\n",
            "Requirement already satisfied: regex>=2022.3.15 in /usr/local/lib/python3.11/dist-packages (from parsimonious<0.11.0,>=0.10.0->eth-abi>=5.0.1->web3) (2024.11.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading web3-7.10.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eth_abi-5.2.0-py3-none-any.whl (28 kB)\n",
            "Downloading eth_account-0.13.6-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m587.0/587.0 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eth_hash-0.7.1-py3-none-any.whl (8.0 kB)\n",
            "Downloading eth_typing-5.2.0-py3-none-any.whl (19 kB)\n",
            "Downloading eth_utils-5.2.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.5/100.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hexbytes-1.3.0-py3-none-any.whl (4.9 kB)\n",
            "Downloading pyunormalize-16.0.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20250328-py3-none-any.whl (20 kB)\n",
            "Downloading bitarray-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (306 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m306.2/306.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ckzg-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (170 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eth_keyfile-0.8.1-py3-none-any.whl (7.5 kB)\n",
            "Downloading eth_keys-0.6.1-py3-none-any.whl (21 kB)\n",
            "Downloading eth_rlp-2.2.0-py3-none-any.whl (4.4 kB)\n",
            "Downloading parsimonious-0.10.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rlp-4.1.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ckzg, bitarray, types-requests, pyunormalize, pycryptodome, parsimonious, hexbytes, eth-typing, eth-hash, cytoolz, eth-utils, rlp, eth-keys, eth-abi, eth-rlp, eth-keyfile, eth-account, web3\n",
            "Successfully installed bitarray-3.3.0 ckzg-2.1.1 cytoolz-1.0.1 eth-abi-5.2.0 eth-account-0.13.6 eth-hash-0.7.1 eth-keyfile-0.8.1 eth-keys-0.6.1 eth-rlp-2.2.0 eth-typing-5.2.0 eth-utils-5.2.0 hexbytes-1.3.0 parsimonious-0.10.0 pycryptodome-3.22.0 pyunormalize-16.0.0 rlp-4.1.0 types-requests-2.32.0.20250328 web3-7.10.0\n"
          ]
        }
      ],
      "source": [
        "pip install web3 opencv-python tensorflow numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import hashlib\n",
        "\n",
        "# -------------------------------\n",
        "# Parameters\n",
        "# -------------------------------\n",
        "IMG_SIZE = 128\n",
        "NUM_FRAMES = 3\n",
        "VIDEO_FOLDER = 'dataset'  # Structure: dataset/real/*.mp4 and dataset/fake/*.mp4\n",
        "\n",
        "# -------------------------------\n",
        "# Generate a unique watermark signal (simulating blockchain hash)\n",
        "# -------------------------------\n",
        "def generate_watermark(content_id):\n",
        "    \"\"\"Simulate blockchain ID hash for watermarking.\"\"\"\n",
        "    hash_digest = hashlib.sha256(content_id.encode()).hexdigest()\n",
        "    binary_hash = bin(int(hash_digest, 16))[2:].zfill(256)\n",
        "    return [int(bit) for bit in binary_hash[:16]]  # Use first 16 bits\n",
        "\n",
        "# -------------------------------\n",
        "# Embed watermark in frames (on fake content)\n",
        "# -------------------------------\n",
        "def embed_watermark(frames, watermark_signal):\n",
        "    \"\"\"Embed watermark bits into the corner pixels of each frame.\"\"\"\n",
        "    for i, bit in enumerate(watermark_signal):\n",
        "        for frame in frames:\n",
        "            row, col = i // 4, i % 4  # 4x4 grid\n",
        "            color = (0, 255, 0) if bit else (0, 0, 255)  # Green for 1, Blue for 0\n",
        "            cv2.rectangle(frame, (col*4, row*4), (col*4 + 3, row*4 + 3), color, -1)\n",
        "    return frames\n",
        "\n",
        "# -------------------------------\n",
        "# Simulated blockchain logging\n",
        "# -------------------------------\n",
        "def log_to_blockchain_simulated(video_name, watermark):\n",
        "    print(f\"ğŸª™ Blockchain Log - {video_name}: Watermark Hash -> {''.join(map(str, watermark))}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Extract frames from a video\n",
        "# -------------------------------\n",
        "def extract_frames(video_path, num_frames=NUM_FRAMES):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"âŒ Error: Could not open video {video_path}\")\n",
        "        return None\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_ids = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    frames = []\n",
        "    for i in range(total_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if i in frame_ids:\n",
        "            frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    if len(frames) < num_frames:\n",
        "        print(f\"âš ï¸ Warning: Only {len(frames)} frames extracted from {video_path}\")\n",
        "        return None\n",
        "    return np.array(frames)\n",
        "\n",
        "# -------------------------------\n",
        "# Extract features using MobileNetV2\n",
        "# -------------------------------\n",
        "def extract_features(frames, feature_extractor):\n",
        "    frames = tf.keras.applications.mobilenet_v2.preprocess_input(frames)\n",
        "    features = feature_extractor.predict(frames, verbose=0)\n",
        "    pooled = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
        "    flattened = tf.reshape(pooled, [-1])\n",
        "    return flattened.numpy()\n",
        "\n",
        "# -------------------------------\n",
        "# Prepare dataset\n",
        "# -------------------------------\n",
        "def prepare_dataset(video_folder, feature_extractor, max_videos=10):\n",
        "    X, y = [], []\n",
        "    for label, folder in enumerate(['real', 'fake']):\n",
        "        folder_path = os.path.join(video_folder, folder)\n",
        "        if not os.path.exists(folder_path):\n",
        "            print(f\"âš ï¸ Warning: Folder {folder_path} not found.\")\n",
        "            continue\n",
        "\n",
        "        video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')][:max_videos]\n",
        "        for video_file in video_files:\n",
        "            video_path = os.path.join(folder_path, video_file)\n",
        "            print(f\"ğŸ“¹ Processing: {video_path}\")\n",
        "            frames = extract_frames(video_path, NUM_FRAMES)\n",
        "            if frames is not None:\n",
        "                # Embed watermark only in fake videos\n",
        "                if label == 1:\n",
        "                    watermark_signal = generate_watermark(video_file)\n",
        "                    frames = embed_watermark(frames, watermark_signal)\n",
        "                    log_to_blockchain_simulated(video_file, watermark_signal)\n",
        "\n",
        "                features = extract_features(frames, feature_extractor)\n",
        "                X.append(features)\n",
        "                y.append(label)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# -------------------------------\n",
        "# Build simple classification model\n",
        "# -------------------------------\n",
        "def build_model(input_shape):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# -------------------------------\n",
        "# Predict on a single video\n",
        "# -------------------------------\n",
        "def predict_video(video_path, trained_model, feature_extractor, num_frames=NUM_FRAMES):\n",
        "    frames = extract_frames(video_path, num_frames)\n",
        "    if frames is None or len(frames) < num_frames:\n",
        "        print(\"âŒ Error: Not enough frames for prediction.\")\n",
        "        return None\n",
        "\n",
        "    features = extract_features(frames, feature_extractor)\n",
        "    prediction = trained_model.predict(np.expand_dims(features, axis=0), verbose=0)[0][0]\n",
        "\n",
        "    label = \"Real Video âœ…\" if prediction < 0.5 else \"Fake Video âŒ\"\n",
        "    confidence = 1 - prediction if prediction < 0.5 else prediction\n",
        "\n",
        "    print(f\"\\nğŸ“Š Prediction Result for: {video_path}\")\n",
        "    print(f\"ğŸ” Status: {label}\")\n",
        "    print(f\"ğŸ“ˆ Confidence Score: {confidence:.4f}\")\n",
        "\n",
        "    return label, confidence\n",
        "\n",
        "# -------------------------------\n",
        "# Main execution\n",
        "# -------------------------------\n",
        "if __name__ == '__main__':\n",
        "    print(\"ğŸš€ Loading MobileNetV2 model...\")\n",
        "    mobilenet_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    print(\"ğŸ“ Preparing dataset...\")\n",
        "    X, y = prepare_dataset(VIDEO_FOLDER, mobilenet_model, max_videos=20)\n",
        "\n",
        "    if len(X) == 0:\n",
        "        print(\"âŒ No features extracted. Please check your dataset structure.\")\n",
        "    else:\n",
        "        print(\"ğŸ§  Training classifier...\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        model = build_model(input_shape=(X_train.shape[1],))\n",
        "        model.fit(X_train, y_train, epochs=10, batch_size=4, validation_data=(X_test, y_test))\n",
        "\n",
        "        print(\"ğŸ’¾ Saving model to 'video_fake_detection_model.h5'\")\n",
        "        model.save('video_fake_detection_model.h5')\n",
        "\n",
        "        # Example prediction\n",
        "        test_video_path = 'dataset/fake/0013_fake.mp4'  # Replace with your test video path\n",
        "        predict_video(test_video_path, model, mobilenet_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52RYv6M-nh71",
        "outputId": "2711ae3e-c635-410f-aedd-2ca162f60637"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Loading MobileNetV2 model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "ğŸ“ Preparing dataset...\n",
            "ğŸ“¹ Processing: dataset/real/0015.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0001.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0023.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0042.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0019.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0039.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0037.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0005.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0028.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0014.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0044.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0027.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0047.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0032.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0036.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0010.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0035.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0000.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0020.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0008.mp4\n",
            "ğŸ“¹ Processing: dataset/fake/0013_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0013_fake.mp4: Watermark Hash -> 1111110001101011\n",
            "ğŸ“¹ Processing: dataset/fake/0047_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0047_fake.mp4: Watermark Hash -> 0000011000100100\n",
            "ğŸ“¹ Processing: dataset/fake/0003_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0003_fake.mp4: Watermark Hash -> 0101111110000011\n",
            "ğŸ“¹ Processing: dataset/fake/0024_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0024_fake.mp4: Watermark Hash -> 0101101010111000\n",
            "ğŸ“¹ Processing: dataset/fake/0000_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0000_fake.mp4: Watermark Hash -> 0011010111001101\n",
            "ğŸ“¹ Processing: dataset/fake/0041_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0041_fake.mp4: Watermark Hash -> 0100101001000010\n",
            "ğŸ“¹ Processing: dataset/fake/0017_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0017_fake.mp4: Watermark Hash -> 1011100000010010\n",
            "ğŸ“¹ Processing: dataset/fake/0028_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0028_fake.mp4: Watermark Hash -> 1101010001100110\n",
            "ğŸ“¹ Processing: dataset/fake/0040_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0040_fake.mp4: Watermark Hash -> 0111111001111001\n",
            "ğŸ“¹ Processing: dataset/fake/0023_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0023_fake.mp4: Watermark Hash -> 1001111111011010\n",
            "ğŸ“¹ Processing: dataset/fake/0015_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0015_fake.mp4: Watermark Hash -> 1001100111111110\n",
            "ğŸ“¹ Processing: dataset/fake/0048_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0048_fake.mp4: Watermark Hash -> 0000101010101001\n",
            "ğŸ“¹ Processing: dataset/fake/0033_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0033_fake.mp4: Watermark Hash -> 0000101111110010\n",
            "ğŸ“¹ Processing: dataset/fake/0044_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0044_fake.mp4: Watermark Hash -> 0111011011110111\n",
            "ğŸ“¹ Processing: dataset/fake/0025_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0025_fake.mp4: Watermark Hash -> 0011101011100111\n",
            "ğŸ“¹ Processing: dataset/fake/0027_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0027_fake.mp4: Watermark Hash -> 0110010100101000\n",
            "ğŸ“¹ Processing: dataset/fake/0019_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0019_fake.mp4: Watermark Hash -> 1110001111010001\n",
            "ğŸ“¹ Processing: dataset/fake/0043_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0043_fake.mp4: Watermark Hash -> 0001100100010011\n",
            "ğŸ“¹ Processing: dataset/fake/0008_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0008_fake.mp4: Watermark Hash -> 0100010111110011\n",
            "ğŸ“¹ Processing: dataset/fake/0011_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0011_fake.mp4: Watermark Hash -> 1010101000110111\n",
            "ğŸ§  Training classifier...\n",
            "Epoch 1/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.4277 - loss: 0.9331 - val_accuracy: 0.2500 - val_loss: 1.0256\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8190 - loss: 0.5465 - val_accuracy: 0.2500 - val_loss: 1.2687\n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7933 - loss: 0.4182 - val_accuracy: 0.2500 - val_loss: 1.4245\n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9191 - loss: 0.1410 - val_accuracy: 0.2500 - val_loss: 2.8654\n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8450 - loss: 0.2776 - val_accuracy: 0.2500 - val_loss: 2.4483\n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9931 - loss: 0.0616 - val_accuracy: 0.2500 - val_loss: 3.0674\n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0579 - val_accuracy: 0.2500 - val_loss: 2.8271\n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0496 - val_accuracy: 0.1250 - val_loss: 3.2205\n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0453 - val_accuracy: 0.1250 - val_loss: 3.3608\n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0123 - val_accuracy: 0.1250 - val_loss: 3.5182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saving model to 'video_fake_detection_model.h5'\n",
            "\n",
            "ğŸ“Š Prediction Result for: dataset/fake/0013_fake.mp4\n",
            "ğŸ” Status: Fake Video âŒ\n",
            "ğŸ“ˆ Confidence Score: 0.9596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import hashlib\n",
        "\n",
        "# -------------------------------\n",
        "# Parameters\n",
        "# -------------------------------\n",
        "IMG_SIZE = 128\n",
        "NUM_FRAMES = 3\n",
        "VIDEO_FOLDER = 'dataset'  # Structure: dataset/real/*.mp4 and dataset/fake/*.mp4\n",
        "\n",
        "# -------------------------------\n",
        "# Generate a unique watermark signal (simulating blockchain hash)\n",
        "# -------------------------------\n",
        "def generate_watermark(content_id):\n",
        "    \"\"\"Simulate blockchain ID hash for watermarking.\"\"\"\n",
        "    hash_digest = hashlib.sha256(content_id.encode()).hexdigest()\n",
        "    binary_hash = bin(int(hash_digest, 16))[2:].zfill(256)\n",
        "    return [int(bit) for bit in binary_hash[:16]]  # Use first 16 bits\n",
        "\n",
        "# -------------------------------\n",
        "# Embed watermark in frames (on fake content)\n",
        "# -------------------------------\n",
        "def embed_watermark(frames, watermark_signal):\n",
        "    \"\"\"Embed watermark bits into the corner pixels of each frame.\"\"\"\n",
        "    for i, bit in enumerate(watermark_signal):\n",
        "        for frame in frames:\n",
        "            row, col = i // 4, i % 4  # 4x4 grid\n",
        "            color = (0, 255, 0) if bit else (0, 0, 255)  # Green for 1, Blue for 0\n",
        "            cv2.rectangle(frame, (col*4, row*4), (col*4 + 3, row*4 + 3), color, -1)\n",
        "    return frames\n",
        "\n",
        "# -------------------------------\n",
        "# Simulated blockchain logging\n",
        "# -------------------------------\n",
        "def log_to_blockchain_simulated(video_name, watermark):\n",
        "    print(f\"ğŸª™ Blockchain Log - {video_name}: Watermark Hash -> {''.join(map(str, watermark))}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Extract frames from a video\n",
        "# -------------------------------\n",
        "def extract_frames(video_path, num_frames=NUM_FRAMES):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"âŒ Error: Could not open video {video_path}\")\n",
        "        return None\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_ids = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    frames = []\n",
        "    for i in range(total_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if i in frame_ids:\n",
        "            frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    if len(frames) < num_frames:\n",
        "        print(f\"âš ï¸ Warning: Only {len(frames)} frames extracted from {video_path}\")\n",
        "        return None\n",
        "    return np.array(frames)\n",
        "\n",
        "# -------------------------------\n",
        "# Extract features using MobileNetV2\n",
        "# -------------------------------\n",
        "def extract_features(frames, feature_extractor):\n",
        "    frames = tf.keras.applications.mobilenet_v2.preprocess_input(frames)\n",
        "    features = feature_extractor.predict(frames, verbose=0)\n",
        "    pooled = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
        "    flattened = tf.reshape(pooled, [-1])\n",
        "    return flattened.numpy()\n",
        "\n",
        "# -------------------------------\n",
        "# Prepare dataset\n",
        "# -------------------------------\n",
        "def prepare_dataset(video_folder, feature_extractor, max_videos=10):\n",
        "    X, y = [], []\n",
        "    for label, folder in enumerate(['real', 'fake']):\n",
        "        folder_path = os.path.join(video_folder, folder)\n",
        "        if not os.path.exists(folder_path):\n",
        "            print(f\"âš ï¸ Warning: Folder {folder_path} not found.\")\n",
        "            continue\n",
        "\n",
        "        video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')][:max_videos]\n",
        "        for video_file in video_files:\n",
        "            video_path = os.path.join(folder_path, video_file)\n",
        "            print(f\"ğŸ“¹ Processing: {video_path}\")\n",
        "            frames = extract_frames(video_path, NUM_FRAMES)\n",
        "            if frames is not None:\n",
        "                # Embed watermark only in fake videos\n",
        "                if label == 1:\n",
        "                    watermark_signal = generate_watermark(video_file)\n",
        "                    frames = embed_watermark(frames, watermark_signal)\n",
        "                    log_to_blockchain_simulated(video_file, watermark_signal)\n",
        "\n",
        "                features = extract_features(frames, feature_extractor)\n",
        "                X.append(features)\n",
        "                y.append(label)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# -------------------------------\n",
        "# Build simple classification model\n",
        "# -------------------------------\n",
        "def build_model(input_shape):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# -------------------------------\n",
        "# Predict on a single video\n",
        "# -------------------------------\n",
        "def predict_video(video_path, trained_model, feature_extractor, num_frames=NUM_FRAMES):\n",
        "    frames = extract_frames(video_path, num_frames)\n",
        "    if frames is None or len(frames) < num_frames:\n",
        "        print(\"âŒ Error: Not enough frames for prediction.\")\n",
        "        return None\n",
        "\n",
        "    features = extract_features(frames, feature_extractor)\n",
        "    prediction = trained_model.predict(np.expand_dims(features, axis=0), verbose=0)[0][0]\n",
        "\n",
        "    label = \"Real Video âœ…\" if prediction < 0.5 else \"Fake Video âŒ\"\n",
        "    confidence = 1 - prediction if prediction < 0.5 else prediction\n",
        "\n",
        "    print(f\"\\nğŸ“Š Prediction Result for: {video_path}\")\n",
        "    print(f\"ğŸ” Status: {label}\")\n",
        "    print(f\"ğŸ“ˆ Confidence Score: {confidence:.4f}\")\n",
        "\n",
        "    return label, confidence\n",
        "\n",
        "# -------------------------------\n",
        "# Main execution\n",
        "# -------------------------------\n",
        "if __name__ == '__main__':\n",
        "    print(\"ğŸš€ Loading MobileNetV2 model...\")\n",
        "    mobilenet_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    print(\"ğŸ“ Preparing dataset...\")\n",
        "    X, y = prepare_dataset(VIDEO_FOLDER, mobilenet_model, max_videos=20)\n",
        "\n",
        "    if len(X) == 0:\n",
        "        print(\"âŒ No features extracted. Please check your dataset structure.\")\n",
        "    else:\n",
        "        print(\"ğŸ§  Training classifier...\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        model = build_model(input_shape=(X_train.shape[1],))\n",
        "        model.fit(X_train, y_train, epochs=10, batch_size=4, validation_data=(X_test, y_test))\n",
        "\n",
        "        print(\"ğŸ’¾ Saving model to 'video_fake_detection_model.h5'\")\n",
        "        model.save('video_fake_detection_model.h5')\n",
        "\n",
        "        # Example prediction\n",
        "        test_video_path = 'dataset/real/0001.mp4'  # Replace with your test video path\n",
        "        predict_video(test_video_path, model, mobilenet_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSZhrCkVnOnw",
        "outputId": "4a9d7847-5e6b-4a6b-f9a9-e35b6a2bc69d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Loading MobileNetV2 model...\n",
            "ğŸ“ Preparing dataset...\n",
            "ğŸ“¹ Processing: dataset/real/0015.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0001.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0023.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0042.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0019.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0039.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0037.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0005.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0028.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0014.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0044.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0027.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0047.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0032.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0036.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0010.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0035.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0000.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0020.mp4\n",
            "ğŸ“¹ Processing: dataset/real/0008.mp4\n",
            "ğŸ“¹ Processing: dataset/fake/0013_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0013_fake.mp4: Watermark Hash -> 1111110001101011\n",
            "ğŸ“¹ Processing: dataset/fake/0047_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0047_fake.mp4: Watermark Hash -> 0000011000100100\n",
            "ğŸ“¹ Processing: dataset/fake/0003_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0003_fake.mp4: Watermark Hash -> 0101111110000011\n",
            "ğŸ“¹ Processing: dataset/fake/0024_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0024_fake.mp4: Watermark Hash -> 0101101010111000\n",
            "ğŸ“¹ Processing: dataset/fake/0000_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0000_fake.mp4: Watermark Hash -> 0011010111001101\n",
            "ğŸ“¹ Processing: dataset/fake/0041_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0041_fake.mp4: Watermark Hash -> 0100101001000010\n",
            "ğŸ“¹ Processing: dataset/fake/0017_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0017_fake.mp4: Watermark Hash -> 1011100000010010\n",
            "ğŸ“¹ Processing: dataset/fake/0028_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0028_fake.mp4: Watermark Hash -> 1101010001100110\n",
            "ğŸ“¹ Processing: dataset/fake/0040_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0040_fake.mp4: Watermark Hash -> 0111111001111001\n",
            "ğŸ“¹ Processing: dataset/fake/0023_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0023_fake.mp4: Watermark Hash -> 1001111111011010\n",
            "ğŸ“¹ Processing: dataset/fake/0015_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0015_fake.mp4: Watermark Hash -> 1001100111111110\n",
            "ğŸ“¹ Processing: dataset/fake/0048_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0048_fake.mp4: Watermark Hash -> 0000101010101001\n",
            "ğŸ“¹ Processing: dataset/fake/0033_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0033_fake.mp4: Watermark Hash -> 0000101111110010\n",
            "ğŸ“¹ Processing: dataset/fake/0044_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0044_fake.mp4: Watermark Hash -> 0111011011110111\n",
            "ğŸ“¹ Processing: dataset/fake/0025_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0025_fake.mp4: Watermark Hash -> 0011101011100111\n",
            "ğŸ“¹ Processing: dataset/fake/0027_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0027_fake.mp4: Watermark Hash -> 0110010100101000\n",
            "ğŸ“¹ Processing: dataset/fake/0019_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0019_fake.mp4: Watermark Hash -> 1110001111010001\n",
            "ğŸ“¹ Processing: dataset/fake/0043_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0043_fake.mp4: Watermark Hash -> 0001100100010011\n",
            "ğŸ“¹ Processing: dataset/fake/0008_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0008_fake.mp4: Watermark Hash -> 0100010111110011\n",
            "ğŸ“¹ Processing: dataset/fake/0011_fake.mp4\n",
            "ğŸª™ Blockchain Log - 0011_fake.mp4: Watermark Hash -> 1010101000110111\n",
            "ğŸ§  Training classifier...\n",
            "Epoch 1/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.3711 - loss: 0.8501 - val_accuracy: 0.6250 - val_loss: 1.2207\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7273 - loss: 0.5602 - val_accuracy: 0.2500 - val_loss: 1.4515\n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7720 - loss: 0.4562 - val_accuracy: 0.3750 - val_loss: 1.5704\n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8837 - loss: 0.2409 - val_accuracy: 0.2500 - val_loss: 1.5373\n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9419 - loss: 0.1540 - val_accuracy: 0.2500 - val_loss: 2.3026\n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9891 - loss: 0.0870 - val_accuracy: 0.1250 - val_loss: 2.4297\n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0797 - val_accuracy: 0.3750 - val_loss: 3.3601\n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9423 - loss: 0.1738 - val_accuracy: 0.6250 - val_loss: 2.8957\n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7837 - loss: 0.6157 - val_accuracy: 0.3750 - val_loss: 4.2973\n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9052 - loss: 0.1910 - val_accuracy: 0.2500 - val_loss: 2.2879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saving model to 'video_fake_detection_model.h5'\n",
            "\n",
            "ğŸ“Š Prediction Result for: dataset/real/0001.mp4\n",
            "ğŸ” Status: Real Video âœ…\n",
            "ğŸ“ˆ Confidence Score: 0.9949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from web3 import Web3\n",
        "import hashlib\n",
        "import json\n",
        "\n",
        "# -------------------------------\n",
        "# Configuration\n",
        "# -------------------------------\n",
        "IMG_SIZE = 128\n",
        "NUM_FRAMES = 3\n",
        "VIDEO_FOLDER = 'dataset'\n",
        "BLOCKCHAIN_RPC = 'https://mainnet.infura.io/v3/03364ceccd3d439a85eb1e91e8ace7f5'\n",
        "WATERMARK_SECRET = \"AI_CONTENT_SIGNATURE\"\n",
        "\n",
        "# -------------------------------\n",
        "# Blockchain Setup\n",
        "# -------------------------------\n",
        "try:\n",
        "    web3 = Web3(Web3.HTTPProvider(BLOCKCHAIN_RPC))\n",
        "    if not web3.is_connected():\n",
        "        raise ConnectionError(\"âš ï¸ Blockchain RPC is unreachable. Using local mock.\")\n",
        "    using_blockchain = True\n",
        "    print(\"âœ… Connected to blockchain.\")\n",
        "except Exception as e:\n",
        "    print(f\"{e}\")\n",
        "    using_blockchain = False\n",
        "    blockchain = {}\n",
        "\n",
        "def register_watermark_on_blockchain(video_id, watermark_hash):\n",
        "    if using_blockchain:\n",
        "        print(f\"ğŸ”— (Simulated) Watermark registered for: {video_id}\")\n",
        "    else:\n",
        "        blockchain[video_id] = watermark_hash\n",
        "        print(f\"ğŸ§ª Local watermark registered: {video_id}\")\n",
        "\n",
        "def verify_watermark_on_blockchain(video_id, watermark_hash):\n",
        "    if using_blockchain:\n",
        "        print(f\"ğŸ” (Simulated) Watermark verified: {video_id}\")\n",
        "        return True  # Simulate always true\n",
        "    else:\n",
        "        return blockchain.get(video_id) == watermark_hash\n",
        "\n",
        "# -------------------------------\n",
        "# Watermark Utilities\n",
        "# -------------------------------\n",
        "def embed_watermark(frame, secret=WATERMARK_SECRET):\n",
        "    overlay = frame.copy()\n",
        "    cv2.putText(overlay, secret, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
        "    return cv2.addWeighted(frame, 0.95, overlay, 0.05, 0)\n",
        "\n",
        "def extract_watermark_hash(frames):\n",
        "    if frames is None or len(frames) == 0:\n",
        "        return None\n",
        "    flattened = np.concatenate([f.flatten() for f in frames])\n",
        "    return hashlib.sha256(flattened).hexdigest()\n",
        "\n",
        "# -------------------------------\n",
        "# Frame Extraction\n",
        "# -------------------------------\n",
        "def extract_frames(video_path, num_frames=NUM_FRAMES, watermark=False):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"âŒ Couldn't open video: {video_path}\")\n",
        "        return None\n",
        "\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total < num_frames:\n",
        "        print(f\"âš ï¸ Not enough frames in {video_path}\")\n",
        "        cap.release()\n",
        "        return None\n",
        "\n",
        "    indexes = np.linspace(0, total - 1, num_frames, dtype=int)\n",
        "    frames = []\n",
        "    current = 0\n",
        "\n",
        "    for i in range(total):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if i in indexes:\n",
        "            frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "            if watermark:\n",
        "                frame = embed_watermark(frame)\n",
        "            frames.append(frame)\n",
        "            current += 1\n",
        "        if current == num_frames:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    if len(frames) < num_frames:\n",
        "        print(f\"âš ï¸ Only {len(frames)} frames extracted from {video_path}\")\n",
        "        return None\n",
        "    return np.array(frames)\n",
        "\n",
        "# -------------------------------\n",
        "# Feature Extraction\n",
        "# -------------------------------\n",
        "def extract_features(frames, feature_extractor):\n",
        "    if frames is None:\n",
        "        return None\n",
        "    pre = tf.keras.applications.mobilenet_v2.preprocess_input(frames)\n",
        "    features = feature_extractor.predict(pre, verbose=0)\n",
        "    pooled = tf.reduce_mean(features, axis=(1, 2))  # Shape: (NUM_FRAMES, channels)\n",
        "    return tf.reshape(pooled, [-1]).numpy()  # Flatten\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset Preparation\n",
        "# -------------------------------\n",
        "def prepare_dataset(folder, extractor, max_videos=50):\n",
        "    X, y = [], []\n",
        "    real_videos = os.listdir(os.path.join(folder, \"real\"))[:max_videos]\n",
        "    fake_videos = os.listdir(os.path.join(folder, \"fake\"))[:max_videos]\n",
        "\n",
        "    for label, video_list, subdir in [(0, real_videos, \"real\"), (1, fake_videos, \"fake\")]:\n",
        "        for v in video_list:\n",
        "            path = os.path.join(folder, subdir, v)\n",
        "            print(f\"ğŸ“¥ Processing {path}\")\n",
        "            apply_wm = (label == 0)\n",
        "            frames = extract_frames(path, watermark=apply_wm)\n",
        "\n",
        "            if frames is None:\n",
        "                print(f\"âš ï¸ Skipping {v} (frame extraction failed)\")\n",
        "                continue\n",
        "\n",
        "            wm_hash = extract_watermark_hash(frames)\n",
        "            if wm_hash is None:\n",
        "                print(f\"âš ï¸ Skipping {v} (hash failed)\")\n",
        "                continue\n",
        "\n",
        "            register_watermark_on_blockchain(v, wm_hash)\n",
        "\n",
        "            features = extract_features(frames, extractor)\n",
        "            if features is None:\n",
        "                print(f\"âš ï¸ Skipping {v} (feature extraction failed)\")\n",
        "                continue\n",
        "\n",
        "            X.append(features)\n",
        "            y.append(label)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Building\n",
        "# -------------------------------\n",
        "def build_model(input_shape):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# -------------------------------\n",
        "# Prediction\n",
        "# -------------------------------\n",
        "def predict_video(video_path, model, extractor):\n",
        "    video_id = os.path.basename(video_path)\n",
        "    frames = extract_frames(video_path, watermark=True)\n",
        "    if frames is None:\n",
        "        print(\"âŒ Frame extraction failed.\")\n",
        "        return\n",
        "\n",
        "    wm_hash = extract_watermark_hash(frames)\n",
        "    if wm_hash is None:\n",
        "        print(\"âŒ Watermark hash extraction failed.\")\n",
        "        return\n",
        "\n",
        "    verified = verify_watermark_on_blockchain(video_id, wm_hash)\n",
        "\n",
        "    features = extract_features(frames, extractor)\n",
        "    if features is None:\n",
        "        print(\"âŒ Feature extraction failed.\")\n",
        "        return\n",
        "\n",
        "    prediction = model.predict(np.expand_dims(features, axis=0), verbose=0)[0][0]\n",
        "\n",
        "    label = \"Real âœ…\" if prediction < 0.5 else \"Fake âŒ\"\n",
        "    confidence = 1 - prediction if prediction < 0.5 else prediction\n",
        "    verification = \"âœ… Verified\" if verified else \"âŒ Not Verified\"\n",
        "\n",
        "    print(f\"\\nğŸ” Prediction: {label}\")\n",
        "    print(f\"ğŸ” Blockchain: {verification}\")\n",
        "    print(f\"ğŸ“Š Confidence: {confidence:.4f}\")\n",
        "    return label, confidence, verification\n",
        "\n",
        "# -------------------------------\n",
        "# Main Execution\n",
        "# -------------------------------\n",
        "if __name__ == '__main__':\n",
        "    print(\"ğŸ“¥ Loading MobileNetV2...\")\n",
        "    extractor = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    print(\"ğŸ“ Preparing dataset...\")\n",
        "    X, y = prepare_dataset(VIDEO_FOLDER, extractor, max_videos=50)\n",
        "\n",
        "    if X.shape[0] == 0:\n",
        "        print(\"âŒ No data loaded.\")\n",
        "        exit()\n",
        "\n",
        "    print(\"ğŸ§  Training model...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = build_model(input_shape=X.shape[1:])\n",
        "    model.fit(X_train, y_train, epochs=15, batch_size=4, validation_data=(X_test, y_test))\n",
        "\n",
        "    model.save(\"video_fake_detection_model.h5\")\n",
        "    print(\"âœ… Model saved as 'video_fake_detection_model.h5'\")\n",
        "\n",
        "    # Inference Test\n",
        "   # test_video = 'dataset/real/0001.mp4'\n",
        "    test_video = 'dataset/fake/0000_fake.mp4'\n",
        "    predict_video(test_video, model, extractor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwKR3y0komYs",
        "outputId": "8d6339fa-25d2-4211-8207-00aab7f8cd9c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Connected to blockchain.\n",
            "ğŸ“¥ Loading MobileNetV2...\n",
            "ğŸ“ Preparing dataset...\n",
            "ğŸ“¥ Processing dataset/real/0015.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0015.mp4\n",
            "ğŸ“¥ Processing dataset/real/0001.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0001.mp4\n",
            "ğŸ“¥ Processing dataset/real/0023.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0023.mp4\n",
            "ğŸ“¥ Processing dataset/real/0042.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0042.mp4\n",
            "ğŸ“¥ Processing dataset/real/0019.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0019.mp4\n",
            "ğŸ“¥ Processing dataset/real/0039.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0039.mp4\n",
            "ğŸ“¥ Processing dataset/real/0037.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0037.mp4\n",
            "ğŸ“¥ Processing dataset/real/0005.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0005.mp4\n",
            "ğŸ“¥ Processing dataset/real/0028.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0028.mp4\n",
            "ğŸ“¥ Processing dataset/real/0014.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0014.mp4\n",
            "ğŸ“¥ Processing dataset/real/0044.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0044.mp4\n",
            "ğŸ“¥ Processing dataset/real/0027.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0027.mp4\n",
            "ğŸ“¥ Processing dataset/real/0047.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0047.mp4\n",
            "ğŸ“¥ Processing dataset/real/0032.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0032.mp4\n",
            "ğŸ“¥ Processing dataset/real/0036.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0036.mp4\n",
            "ğŸ“¥ Processing dataset/real/0010.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0010.mp4\n",
            "ğŸ“¥ Processing dataset/real/0035.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0035.mp4\n",
            "ğŸ“¥ Processing dataset/real/0000.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0000.mp4\n",
            "ğŸ“¥ Processing dataset/real/0020.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0020.mp4\n",
            "ğŸ“¥ Processing dataset/real/0008.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0008.mp4\n",
            "ğŸ“¥ Processing dataset/real/0030.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0030.mp4\n",
            "ğŸ“¥ Processing dataset/real/0033.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0033.mp4\n",
            "ğŸ“¥ Processing dataset/real/0031.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0031.mp4\n",
            "ğŸ“¥ Processing dataset/real/0045.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0045.mp4\n",
            "ğŸ“¥ Processing dataset/real/0009.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0009.mp4\n",
            "ğŸ“¥ Processing dataset/real/0013.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0013.mp4\n",
            "ğŸ“¥ Processing dataset/real/0011.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0011.mp4\n",
            "ğŸ“¥ Processing dataset/real/0002.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0002.mp4\n",
            "ğŸ“¥ Processing dataset/real/0046.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0046.mp4\n",
            "ğŸ“¥ Processing dataset/real/0017.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0017.mp4\n",
            "ğŸ“¥ Processing dataset/real/0003.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0003.mp4\n",
            "ğŸ“¥ Processing dataset/real/0024.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0024.mp4\n",
            "ğŸ“¥ Processing dataset/real/0048.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0048.mp4\n",
            "ğŸ“¥ Processing dataset/real/0004.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0004.mp4\n",
            "ğŸ“¥ Processing dataset/real/0007.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0007.mp4\n",
            "ğŸ“¥ Processing dataset/real/0038.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0038.mp4\n",
            "ğŸ“¥ Processing dataset/real/0026.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0026.mp4\n",
            "ğŸ“¥ Processing dataset/real/0012.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0012.mp4\n",
            "ğŸ“¥ Processing dataset/real/0034.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0034.mp4\n",
            "ğŸ“¥ Processing dataset/real/0022.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0022.mp4\n",
            "ğŸ“¥ Processing dataset/real/0041.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0041.mp4\n",
            "ğŸ“¥ Processing dataset/real/0016.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0016.mp4\n",
            "ğŸ“¥ Processing dataset/real/0018.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0018.mp4\n",
            "ğŸ“¥ Processing dataset/real/0025.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0025.mp4\n",
            "ğŸ“¥ Processing dataset/real/0040.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0040.mp4\n",
            "ğŸ“¥ Processing dataset/real/0021.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0021.mp4\n",
            "ğŸ“¥ Processing dataset/real/0043.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0043.mp4\n",
            "ğŸ“¥ Processing dataset/real/0029.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0029.mp4\n",
            "ğŸ“¥ Processing dataset/real/0006.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0006.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0013_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0013_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0047_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0047_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0003_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0003_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0024_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0024_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0000_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0000_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0041_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0041_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0017_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0017_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0028_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0028_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0040_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0040_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0023_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0023_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0015_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0015_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0048_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0048_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0033_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0033_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0044_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0044_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0025_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0025_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0027_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0027_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0019_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0019_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0043_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0043_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0008_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0008_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0011_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0011_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0012_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0012_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0042_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0042_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0029_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0029_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0004_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0004_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0020_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0020_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0022_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0022_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0001_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0001_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0010_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0010_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0005_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0005_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0035_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0035_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0014_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0014_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0007_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0007_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0009_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0009_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0031_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0031_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0002_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0002_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0006_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0006_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0018_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0018_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0039_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0039_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0038_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0038_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0021_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0021_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0045_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0045_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0026_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0026_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0037_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0037_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0032_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0032_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0016_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0016_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0036_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0036_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0046_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0046_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0034_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0034_fake.mp4\n",
            "ğŸ“¥ Processing dataset/fake/0030_fake.mp4\n",
            "ğŸ”— (Simulated) Watermark registered for: 0030_fake.mp4\n",
            "ğŸ§  Training model...\n",
            "Epoch 1/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.4946 - loss: 1.3278 - val_accuracy: 0.4500 - val_loss: 0.8491\n",
            "Epoch 2/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.5243 - loss: 1.8836 - val_accuracy: 0.3500 - val_loss: 0.9243\n",
            "Epoch 3/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6577 - loss: 0.9206 - val_accuracy: 0.6000 - val_loss: 1.0470\n",
            "Epoch 4/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5460 - loss: 1.1555 - val_accuracy: 0.5500 - val_loss: 0.8283\n",
            "Epoch 5/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.5309 - loss: 1.0812 - val_accuracy: 0.3500 - val_loss: 1.0243\n",
            "Epoch 6/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.4295 - loss: 1.1724 - val_accuracy: 0.2500 - val_loss: 0.9260\n",
            "Epoch 7/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5859 - loss: 0.9498 - val_accuracy: 0.3000 - val_loss: 0.7920\n",
            "Epoch 8/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5327 - loss: 0.7546 - val_accuracy: 0.2500 - val_loss: 0.8737\n",
            "Epoch 9/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6538 - loss: 0.7006 - val_accuracy: 0.3000 - val_loss: 0.9205\n",
            "Epoch 10/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7339 - loss: 0.5305 - val_accuracy: 0.3000 - val_loss: 1.2512\n",
            "Epoch 11/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6265 - loss: 0.7754 - val_accuracy: 0.1500 - val_loss: 1.0154\n",
            "Epoch 12/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7263 - loss: 0.5920 - val_accuracy: 0.3500 - val_loss: 1.0804\n",
            "Epoch 13/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.6633 - loss: 0.5630 - val_accuracy: 0.3000 - val_loss: 1.1187\n",
            "Epoch 14/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6874 - loss: 0.5753 - val_accuracy: 0.3000 - val_loss: 1.2717\n",
            "Epoch 15/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7910 - loss: 0.4942 - val_accuracy: 0.1500 - val_loss: 1.1232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model saved as 'video_fake_detection_model.h5'\n",
            "ğŸ” (Simulated) Watermark verified: 0000_fake.mp4\n",
            "\n",
            "ğŸ” Prediction: Fake âŒ\n",
            "ğŸ” Blockchain: âœ… Verified\n",
            "ğŸ“Š Confidence: 0.7374\n"
          ]
        }
      ]
    }
  ]
}