{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arlakhan/Deep-Fake-Detection/blob/main/Video_Fake_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -------------------------------\n",
        "# Parameters\n",
        "# -------------------------------\n",
        "IMG_SIZE = 128\n",
        "NUM_FRAMES = 3\n",
        "VIDEO_FOLDER = 'dataset'  # Structure: dataset/real/*.mp4 and dataset/fake/*.mp4\n",
        "\n",
        "# -------------------------------\n",
        "# Extract frames from a video\n",
        "# -------------------------------\n",
        "def extract_frames(video_path, num_frames=NUM_FRAMES):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"‚ùå Error: Could not open video {video_path}\")\n",
        "        return None\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_ids = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    frames = []\n",
        "    for i in range(total_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if i in frame_ids:\n",
        "            frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    if len(frames) < num_frames:\n",
        "        print(f\"‚ö†Ô∏è Warning: Only {len(frames)} frames extracted from {video_path}\")\n",
        "        return None\n",
        "    return np.array(frames)\n",
        "\n",
        "# -------------------------------\n",
        "# Extract features using MobileNetV2\n",
        "# -------------------------------\n",
        "def extract_features(frames, feature_extractor):\n",
        "    frames = tf.keras.applications.mobilenet_v2.preprocess_input(frames)\n",
        "    features = feature_extractor.predict(frames, verbose=0)\n",
        "    pooled = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
        "    flattened = tf.reshape(pooled, [-1])\n",
        "    return flattened.numpy()\n",
        "\n",
        "# -------------------------------\n",
        "# Prepare dataset\n",
        "# -------------------------------\n",
        "def prepare_dataset(video_folder, feature_extractor, max_videos=10):\n",
        "    X, y = [], []\n",
        "    for label, folder in enumerate(['real', 'fake']):\n",
        "        folder_path = os.path.join(video_folder, folder)\n",
        "        if not os.path.exists(folder_path):\n",
        "            print(f\"‚ö†Ô∏è Warning: Folder {folder_path} not found.\")\n",
        "            continue\n",
        "\n",
        "        video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')][:max_videos]\n",
        "        for video_file in video_files:\n",
        "            video_path = os.path.join(folder_path, video_file)\n",
        "            print(f\"üìπ Processing: {video_path}\")\n",
        "            frames = extract_frames(video_path, NUM_FRAMES)\n",
        "            if frames is not None:\n",
        "                features = extract_features(frames, feature_extractor)\n",
        "                X.append(features)\n",
        "                y.append(label)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# -------------------------------\n",
        "# Build simple classification model\n",
        "# -------------------------------\n",
        "def build_model(input_shape):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# -------------------------------\n",
        "# Predict on a single video\n",
        "# -------------------------------\n",
        "def predict_video(video_path, trained_model, feature_extractor, num_frames=NUM_FRAMES):\n",
        "    frames = extract_frames(video_path, num_frames)\n",
        "    if frames is None or len(frames) < num_frames:\n",
        "        print(\"‚ùå Error: Not enough frames for prediction.\")\n",
        "        return None\n",
        "\n",
        "    features = extract_features(frames, feature_extractor)\n",
        "    prediction = trained_model.predict(np.expand_dims(features, axis=0), verbose=0)[0][0]\n",
        "\n",
        "    label = \"Real Video ‚úÖ\" if prediction < 0.5 else \"Fake Video ‚ùå\"\n",
        "    confidence = 1 - prediction if prediction < 0.5 else prediction\n",
        "\n",
        "    print(f\"\\nüìä Prediction Result for: {video_path}\")\n",
        "    print(f\"üîé Status: {label}\")\n",
        "    print(f\"üìà Confidence Score: {confidence:.4f}\")\n",
        "\n",
        "    return label, confidence\n",
        "\n",
        "# -------------------------------\n",
        "# Main execution\n",
        "# -------------------------------\n",
        "if __name__ == '__main__':\n",
        "    print(\"üöÄ Loading MobileNetV2 model...\")\n",
        "    mobilenet_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    print(\"üìÅ Preparing dataset...\")\n",
        "    X, y = prepare_dataset(VIDEO_FOLDER, mobilenet_model, max_videos=20)\n",
        "\n",
        "    if len(X) == 0:\n",
        "        print(\"‚ùå No features extracted. Please check your dataset structure.\")\n",
        "    else:\n",
        "        print(\"üß† Training classifier...\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        model = build_model(input_shape=(X_train.shape[1],))\n",
        "        model.fit(X_train, y_train, epochs=10, batch_size=4, validation_data=(X_test, y_test))\n",
        "\n",
        "        print(\"üíæ Saving model to 'video_fake_detection_model.h5'\")\n",
        "        model.save('video_fake_detection_model.h5')\n",
        "\n",
        "        # Example prediction\n",
        "        test_video_path = 'dataset/fake/0003_fake.mp4'  # Replace with your test video path\n",
        "        #test_video_path = 'dataset/real/0001.mp4'  # Replace with your test video path\n",
        "        predict_video(test_video_path, model, mobilenet_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcMmqjz1ULrB",
        "outputId": "35d3e609-ba7c-4013-a75f-d250007d7d25"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Loading MobileNetV2 model...\n",
            "üìÅ Preparing dataset...\n",
            "üìπ Processing: dataset/real/0041.mp4\n",
            "üìπ Processing: dataset/real/0032.mp4\n",
            "üìπ Processing: dataset/real/0024.mp4\n",
            "üìπ Processing: dataset/real/0027.mp4\n",
            "üìπ Processing: dataset/real/0004.mp4\n",
            "üìπ Processing: dataset/real/0016.mp4\n",
            "üìπ Processing: dataset/real/0013.mp4\n",
            "üìπ Processing: dataset/real/0047.mp4\n",
            "üìπ Processing: dataset/real/0009.mp4\n",
            "üìπ Processing: dataset/real/0040.mp4\n",
            "üìπ Processing: dataset/real/0001.mp4\n",
            "üìπ Processing: dataset/real/0010.mp4\n",
            "üìπ Processing: dataset/real/0025.mp4\n",
            "üìπ Processing: dataset/real/0031.mp4\n",
            "üìπ Processing: dataset/real/0042.mp4\n",
            "üìπ Processing: dataset/real/0017.mp4\n",
            "üìπ Processing: dataset/real/0008.mp4\n",
            "üìπ Processing: dataset/real/0033.mp4\n",
            "üìπ Processing: dataset/real/0036.mp4\n",
            "üìπ Processing: dataset/real/0035.mp4\n",
            "üìπ Processing: dataset/fake/0038_fake.mp4\n",
            "üìπ Processing: dataset/fake/0003_fake.mp4\n",
            "üìπ Processing: dataset/fake/0010_fake.mp4\n",
            "üìπ Processing: dataset/fake/0005_fake.mp4\n",
            "üìπ Processing: dataset/fake/0000_fake.mp4\n",
            "üìπ Processing: dataset/fake/0018_fake.mp4\n",
            "üìπ Processing: dataset/fake/0030_fake.mp4\n",
            "üìπ Processing: dataset/fake/0020_fake.mp4\n",
            "üìπ Processing: dataset/fake/0023_fake.mp4\n",
            "üìπ Processing: dataset/fake/0036_fake.mp4\n",
            "üìπ Processing: dataset/fake/0001_fake.mp4\n",
            "üìπ Processing: dataset/fake/0037_fake.mp4\n",
            "üìπ Processing: dataset/fake/0027_fake.mp4\n",
            "üìπ Processing: dataset/fake/0006_fake.mp4\n",
            "üìπ Processing: dataset/fake/0019_fake.mp4\n",
            "üìπ Processing: dataset/fake/0035_fake.mp4\n",
            "üìπ Processing: dataset/fake/0041_fake.mp4\n",
            "üìπ Processing: dataset/fake/0040_fake.mp4\n",
            "üìπ Processing: dataset/fake/0008_fake.mp4\n",
            "üìπ Processing: dataset/fake/0007_fake.mp4\n",
            "üß† Training classifier...\n",
            "Epoch 1/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.4505 - loss: 0.9361 - val_accuracy: 0.3750 - val_loss: 1.2072\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5581 - loss: 0.6708 - val_accuracy: 0.3750 - val_loss: 1.3822\n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8336 - loss: 0.3691 - val_accuracy: 0.2500 - val_loss: 2.9591\n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7820 - loss: 0.3352 - val_accuracy: 0.3750 - val_loss: 1.9835\n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9597 - loss: 0.1556 - val_accuracy: 0.2500 - val_loss: 2.8761\n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9564 - loss: 0.1203 - val_accuracy: 0.2500 - val_loss: 2.5283\n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9055 - loss: 0.1895 - val_accuracy: 0.1250 - val_loss: 3.1709\n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9472 - loss: 0.1401 - val_accuracy: 0.3750 - val_loss: 2.6239\n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9168 - loss: 0.1375 - val_accuracy: 0.2500 - val_loss: 4.6041\n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8580 - loss: 0.2126 - val_accuracy: 0.3750 - val_loss: 2.7661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving model to 'video_fake_detection_model.h5'\n",
            "\n",
            "üìä Prediction Result for: dataset/fake/0003_fake.mp4\n",
            "üîé Status: Fake Video ‚ùå\n",
            "üìà Confidence Score: 0.9660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KBf3f5HTMMBv"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}